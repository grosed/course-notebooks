{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee33a35c",
   "metadata": {},
   "source": [
    "# M550 Python Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af33de4",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a073c09c",
   "metadata": {},
   "source": [
    "This is the assessment for the M550 Python Module. It counts as 30% of your overall mark for M550.\n",
    "\n",
    "The assessment is to be undertaken in small groups and your work should be submitted in the form of a single jupyter notebook.\n",
    "\n",
    "The submission date for this assessment is 16/11/2021. Your jupyter notebook should be emailed directly  to [daniel grose](mailto:dan.grose@lancaster.ac.uk) by 17:00 on this date (one copy per group).\n",
    "\n",
    "Before your results and feedback are returned you might be asked to have a short (approximately 5 minutes) individual online \"interview\" to discuss some aspects of your work. The outcome of this interview might impact on your overall individual score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f40a0fd",
   "metadata": {},
   "source": [
    "### <u>Task 1</u>\n",
    "\n",
    "Confirm the names of all of the members of your group along with their e-mail addresses to [daniel grose](mailto:dan.grose@lancaster.ac.uk) no later than 17:00 on 04/11/2021. Only one mamber of the group has to send the e-mail. You need to complete this task to enable the other tasks to be assessed.\n",
    "\n",
    "__No marks for this task__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb809a9a",
   "metadata": {},
   "source": [
    "The assessment asks you to study some sections of a short chapter from the book [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) by Daniel Jurafsky & James H. Martin. A pdf version of the chapter can be downloaded <a href=\"./3rd-party/speech-and-language-processing-c4.pdf\" download>here</a>. The assessment will guide you through this reading.\n",
    "\n",
    "The assessment is subdivided into a series of tasks most of which have a mark associated with them. You should look over all of the tasks before you start working through them so as to be able to decide how to share the work amongst your group. For many of the tasks you may want to attempt the task yourself (at least in part) and then compare your work with that of your other group members. You can then combine the best elements of your individual solutions/ideas into your group jupyter notebook. Make sure you use plenty of markdown to enhance you answers and provide code examples (that work) to support you findings.\n",
    "\n",
    "Task 12 is slightly different from the other tasks in that it provides a list of suggestions as to possible ways you could extend and enhance your work. Bear in mind these are only suggestions, and you might have your own ideas as to how to improve the methods and tasks covered in the task. Note that you should not feel that you have to pursue all of the suggestions !!\n",
    "\n",
    "To help with tasks, and in particular task 12, a number of code examples have been provided in the appendix. These should help you gain some insight to how to complete the tasks using python. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7ffa1b",
   "metadata": {},
   "source": [
    "## Naive Bayes and Sentiment Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee86509",
   "metadata": {},
   "source": [
    "### <u>Task 2</u>\n",
    "\n",
    "Read chapter 4 of [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) (pdf version available <a href=\"./3rd-party/speech-and-language-processing-c4.pdf\" download>here</a>) up to the end of section 4.1.\n",
    "\n",
    "__No marks for this task__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a9f422",
   "metadata": {},
   "source": [
    "### <u>Task 3</u>\n",
    "\n",
    "A python dictionary can be used to organise the information summarised in figure 4.1 in [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/). For example :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1aafe51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it': 6, 'I': 5, 'the': 4}\n"
     ]
    }
   ],
   "source": [
    "document = {\"it\" : 6,\"I\" : 5, \"the\" : 4}\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024fcc97",
   "metadata": {},
   "source": [
    "However, a document is often provided as a list of words, not a dictionary. Write a function that takes a list of words and returns a dictionary with the words as keys and the word frequencies as values. Test your code using the following list of strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a12f3613",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = [\"fish\",\"dog\",\"cat\",\"fish\",\"cat\",\"fish\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f5b2e3",
   "metadata": {},
   "source": [
    "Explain what each line of your function does.\n",
    "\n",
    "Explain why a python dictionary a good choice of data structure to use for a document. If you are not sure, do some online research. Note that a python dictionary is an example of a more general data structure called a __hash map__.\n",
    "\n",
    "You may find the examples and notes in the __dictionaries__ chapter helpful\n",
    "\n",
    "__Marks__ [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e7b7a0",
   "metadata": {},
   "source": [
    "## Training the Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d2f283",
   "metadata": {},
   "source": [
    "### <u>Task 4</u>\n",
    "\n",
    "Read section 4.1 of [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) up to the end of the paragraph after equation 4.14.\n",
    "\n",
    "__No marks for this task__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc0bf7",
   "metadata": {},
   "source": [
    "### <u>Task 5</u>\n",
    "\n",
    "Use your solution to __Task 3__ to create documents out of the following strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d890757",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"just plain boring\"\n",
    "s2 = \"entirely predictable and lacks energy\"\n",
    "s3 = \"no surprises and very few laughs\"\n",
    "s4 = \"very powerful\"\n",
    "s5 = \"the most fun film of the summer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14a26df",
   "metadata": {},
   "source": [
    "You can use the __split__ function to create a list of strings from the strings given above. For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f2e0374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['just', 'plain', 'boring']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6f5636",
   "metadata": {},
   "source": [
    "__Marks__ [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad20078f",
   "metadata": {},
   "source": [
    "### <u>Task 6</u>\n",
    "\n",
    "A category, as described in [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/), has the same structure as a document, and can be constructed by combining documents together. This is also true of a vocabulary.\n",
    "\n",
    "Write a function to create a document (category,vocabulary) from a list of other documents.  Explain how your function works.\n",
    "\n",
    "You might find the chapter on __lists__ useful.\n",
    "\n",
    "__Marks__ [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2f4c08",
   "metadata": {},
   "source": [
    "### <u>Task 7</u>\n",
    "\n",
    "Using the strings introduced in __Task 5__ and your solution to __Task 6__,  create a category __negative__ from strings s1, s2, and s3, and a category __positive__ from s4 and s5. Create a __vocabulary__ from the __negative__ and __positive__ categories. \n",
    "\n",
    "Display the categories and vocabulary you have created.\n",
    "\n",
    "__Marks__ [5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ed68c",
   "metadata": {},
   "source": [
    "### <u>Task 8</u>\n",
    "\n",
    "Verify that the following function implements equation 4.14 in [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70604a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_word_given_category(vocab,category,word) :\n",
    "    denom = 0\n",
    "    for entry in vocab:\n",
    "        if category.get(entry) != None :\n",
    "            denom +=  category[entry]\n",
    "    denom += len(vocab)\n",
    "    num = 1\n",
    "    if category.get(word) != None :\n",
    "        num += category[word]\n",
    "    prob = num/denom\n",
    "    return(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7726b5",
   "metadata": {},
   "source": [
    "Describe what each line of the function does with reference to equation 4.14 and add some suitable comments to the function.\n",
    "\n",
    "__Marks__ [5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f21d1e",
   "metadata": {},
   "source": [
    "### <u> Task 9</u>\n",
    "\n",
    "Use the function in __Task 8__ and your solution to __Task 7__ to verify the worked example in section 4.3 of [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/).\n",
    "\n",
    "__Marks__ [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c21961e",
   "metadata": {},
   "source": [
    "### <u> Task 10</u>\n",
    "\n",
    "Use your solutions to the proceeding tasks to solve exercise 4.2 in [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/).\n",
    "\n",
    "__Marks__ [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618d1b29",
   "metadata": {},
   "source": [
    "## Document classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4c090e",
   "metadata": {},
   "source": [
    "The methods introduced in sections 4.1 and 4.2 of [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) can be used to classify documents according to their content. To do this, several documents of a known type (i.e. science fiction novel, medical journal) are used create categories and a vocabulary, and then \n",
    "individual documents are classified accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fada6f",
   "metadata": {},
   "source": [
    "### <u>Task 11</u>\n",
    "\n",
    "Download at least 20 examples of work (chapters, acts, sections etc) from at least 4 different authors (of your choosing) from [project gutenberg](https://www.gutenberg.org/). Use the examples to create documents, categories, and a vocabulary for the purposes of classification. \n",
    "\n",
    "You will probably want to modify the functions you used in the earlier tasks to process the text you obtain from project gutenberg more effectively. Have a look at the code examples in the appendix to help you decide how this might be done. \n",
    "\n",
    "__Marks__ [25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f4d464",
   "metadata": {},
   "source": [
    "### <u>Task 12</u>\n",
    "\n",
    "Use your results from previous tasks and equation 4.10 in [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) to try and categorise excerpts of text from works by the authors you chose in __Task 11__. The example code in the appendix should help you achieve this. Note - what probability do you assign a word that is in a excerpt but not in the vocabulary ? What effect (if any) does the length of the excerpt have ?\n",
    "\n",
    "Summarise your findings.\n",
    "\n",
    "Try and improve your classification method by considering some of the following.\n",
    "\n",
    "- Process your documents using a stemmer\n",
    "- Remove words that are not in an english dictionary\n",
    "- Find a way of ranking the words in your vocabulary and categories and use this to remove some of the most common and least common words.\n",
    "- Explore the __multinomial naive Bayes__ method suggested in section 4.4 of [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/).\n",
    "- Any good idea you have (or have found out about) that might improve the ability to classify your excerpts.\n",
    "\n",
    "__Marks__ [20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f10f2d",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ad0db1",
   "metadata": {},
   "source": [
    "### <u>nltk package</u>\n",
    "\n",
    "This is a useful package for helping with document classification. You can install it using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d587329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/grosedj/python-envs/M550/env/lib/python3.9/site-packages (3.6.5)\n",
      "Requirement already satisfied: click in /home/grosedj/python-envs/M550/env/lib/python3.9/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/grosedj/python-envs/M550/env/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/grosedj/python-envs/M550/env/lib/python3.9/site-packages (from nltk) (2021.10.23)\n",
      "Requirement already satisfied: tqdm in /home/grosedj/python-envs/M550/env/lib/python3.9/site-packages (from nltk) (4.62.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/grosedj/python-envs/M550/env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d85169d",
   "metadata": {},
   "source": [
    "### using nltk to check for english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9b96b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/grosedj/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hello', 'fish', 'go', 'vis']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "english = set(nltk.corpus.words.words())\n",
    "\n",
    "some_words = [\"hello\",\"fish\",\"go\",\"vis\",\"bonjour\",\"gedaan\"]\n",
    "english_words = [word for word in some_words if word in english]\n",
    "english_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c02dea",
   "metadata": {},
   "source": [
    "### reading a file of text as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42491777",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"./data/j-austen-persuasion-c1.txt\"\n",
    "file = open(file_name, \"r\")\n",
    "content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dafe1b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### using nltk to find the stems of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "157cec29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'love', 'love', 'quick', 'quicker', 'quick']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stemmer = nltk.SnowballStemmer(language='english')\n",
    "\n",
    "words = [\"love\",\"lovely\",\"loved\",\"quick\",\"quicker\",\"quickly\"]\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed0b12e",
   "metadata": {},
   "source": [
    "### using re to split a string on more than one delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72750b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'string', 'to', '', 'test', '', 'some', '', '', 'of', \"re's\", ':', 'delimiters']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "s = \"a string to, test, some \\n of re's : delimiters\"\n",
    "words = re.split(' |\\n|,|\\\"',s)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9ea79a",
   "metadata": {},
   "source": [
    "Note - how would you get rid of those '' entries in words ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
